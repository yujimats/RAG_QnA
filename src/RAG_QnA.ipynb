{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QnA RAG\n",
    "Q&A集を参照し、RAGで検索するプログラム。  \n",
    "`RAG_QnA/docker/README.md`を参照し、Dockerコンテナ内で実行すること。  \n",
    "\n",
    "データは日本の官公庁のWebサイトに掲載されている「よくある質問」を用いている。  \n",
    "データ整備に尽力頂いたmatsuxr様に感謝する。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインストール\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import tiktoken\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# initial settings\n",
    "load_dotenv()\n",
    "API_KEYS = os.environ['API_KEYS']\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "openai.api_key = API_KEYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIキーの動作確認\n",
    "APIキーの動作確認を行う  \n",
    "方法は様々だが、ここでは'hello world'の`embedding`を生成するという方法で確認してみる。  \n",
    "ベクトルが表示されればOK。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_text(texts):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        response = openai.Embedding.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=batch\n",
    "        )\n",
    "        embeddings.extend(response['data'])\n",
    "    return embeddings\n",
    "\n",
    "emb = create_embeddings_text('hello world')\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `token`を確認\n",
    "入力できる`token`数には制限があるため、事前に確認する関数を用意  \n",
    "ここで用いている`text-embedding-ada-002`の最大`token`数は`4097`  \n",
    "データは簡単のため気象庁が発行したものに絞っている。  \n",
    "幸い、気象庁が発行するQ&Aでは、`token`が4097をこえるものは無かった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenを測定する関数を用意\n",
    "def num_tokens(text, model=GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# data.jsonlをDataFrameで読み込み\n",
    "path_data = os.path.join('data', 'data.jsonl')\n",
    "df_data = pd.read_json(path_data, orient='records', lines=True)\n",
    "\n",
    "# 'copyright'が気象庁のものだけピックアップ\n",
    "df_data = df_data[df_data['copyright']=='気象庁']\n",
    "\n",
    "# QuestionとAnswerを結合\n",
    "df_data['QnA'] = df_data['Question'] + ' ' + df_data['Answer']\n",
    "# token取得\n",
    "df_data['token_QnA'] = df_data['QnA'].apply(num_tokens)\n",
    "\n",
    "# tokenが4097以上のものは除外\n",
    "df_data = df_data[df_data['token_QnA'] < 4097].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Embeddings`の生成\n",
    "ベクトル検索をかけるための`Embeddings`を生成する。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(items):\n",
    "    embeddings = []\n",
    "    for batch_start in range(0, len(items), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = items[batch_start:batch_end]\n",
    "        print(f'Batch {batch_start} to {batch_end-1}')\n",
    "        response = openai.Embedding.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=batch\n",
    "        )\n",
    "        for i, be in enumerate(response['data']):\n",
    "            assert i == be['index'] # double check embeddings are in same order as input\n",
    "        batch_embeddings = [e['embedding'] for e in response['data']]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    df = pd.DataFrame({'text': items, 'embedding': embeddings})\n",
    "    return df\n",
    "\n",
    "# Embeddings\n",
    "items = df_data['QnA'].to_list()\n",
    "df_embedding = create_embeddings(items=items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベース設定\n",
    "データベースを設定する。  \n",
    "ここでは`chromadb`を採用する。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_client():\n",
    "    persist_directory = 'chroma_persistence'\n",
    "    chroma_client = chromadb.Client(\n",
    "        Settings(\n",
    "            persist_directory=persist_directory,\n",
    "            chroma_db_impl=\"duckdb+parquet\",\n",
    "        )\n",
    "    )\n",
    "    return chroma_client\n",
    "\n",
    "def chroma_collection(chroma_client):\n",
    "    chroma_client.reset()\n",
    "    collection_name = 'stevie_collection'\n",
    "    embedding_function = OpenAIEmbeddingFunction(api_key=API_KEYS, model_name=EMBEDDING_MODEL)\n",
    "    collection = chroma_client.create_collection(name=collection_name, embedding_function=embedding_function)\n",
    "    return collection\n",
    "\n",
    "# settings for chromacb\n",
    "chroma_client = create_chroma_client()\n",
    "stevei_collection = chroma_collection(chroma_client)\n",
    "stevei_collection.add(\n",
    "    ids = df_embedding.index.astype(str).tolist(),\n",
    "    documents=df_embedding['text'].tolist(),\n",
    "    embeddings=df_embedding['embedding'].tolist(),\n",
    ")\n",
    "chroma_client.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データベースを検索する\n",
    "ベクトル検索の設定。  \n",
    "応答に加え、関連度も出力する。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(\n",
    "        query: str,\n",
    "        collection: chromadb.api.models.Collection.Collection,\n",
    "        max_results: int=100) -> tuple[list[str], list[float]]:\n",
    "    results = collection.query(query_texts=query, n_results=max_results, include=['documents', 'distances'])\n",
    "    strings = results['documents'][0]\n",
    "    relatednesses = [1 - x for x in results['distances'][0]]\n",
    "    return strings, relatednesses\n",
    "\n",
    "# response\n",
    "strings, relatednesses = query_collection(\n",
    "    collection=stevei_collection,\n",
    "    query='昨今の異常気象の原因を教えて下さい',\n",
    "    max_results=3,\n",
    ")\n",
    "\n",
    "for string, relatednesses in zip(strings, relatednesses):\n",
    "    print(f'{relatednesses=:.3f}')\n",
    "    print(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
